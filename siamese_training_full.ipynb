{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "A4pq_tDukMYb"
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import keras.utils\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.applications import mobilenet, resnet50, inception_resnet_v2, inception_v3, vgg16\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import dot, Dropout, Dense, GlobalAveragePooling2D, Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTWon4JLkMYs"
   },
   "source": [
    "## Load Training / Validation Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IVvMCzYLkMYz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train encodings: (122400, 2, 2048)\n",
      "x_val encodings: (13600, 2, 2048)\n"
     ]
    }
   ],
   "source": [
    "with open('x_train_pairs_resnet.pickle', 'rb') as f:\n",
    "    x_train_encoding = pickle.load(file=f)\n",
    "\n",
    "with open('x_val_pairs_resnet.pickle', 'rb') as f:\n",
    "    x_val_encoding = pickle.load(file=f)\n",
    "\n",
    "print(\"x_train encodings: \" + str(x_train_encoding.shape))\n",
    "print(\"x_val encodings: \" + str(x_val_encoding.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CnVaUClkkMZI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: (122400,)\n",
      "y_val: (13600,)\n"
     ]
    }
   ],
   "source": [
    "def create_labels(n_labels):\n",
    "    \n",
    "    y = []\n",
    "    \n",
    "    for i in range(0,n_labels):\n",
    "        if i%2==0: \n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "            \n",
    "    return np.array(y, np.int)\n",
    "\n",
    "y_train = create_labels(x_train_encoding.shape[0])\n",
    "y_val = create_labels(x_val_encoding.shape[0])\n",
    "print(\"y_train: \"+str(y_train.shape))\n",
    "print(\"y_val: \"+str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfRDyIZ9kMZU"
   },
   "source": [
    "## Define Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WZNOIsd4kMZZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoding_a (InputLayer)         (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoding_b (InputLayer)         (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 2048)         0           encoding_a[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 2048)         0           encoding_b[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc1_a (Dense)                   (None, 4096)         8392704     dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc1_b (Dense)                   (None, 4096)         8392704     dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "cos_distance (Lambda)           (None, 1)            0           fc1_a[0][0]                      \n",
      "                                                                 fc1_b[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sigmoid (Dense)                 (None, 1)            2           cos_distance[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 16,785,410\n",
      "Trainable params: 16,785,410\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# calculate cosine distance b/t feature vector outputs from base network\n",
    "def cos_distance(feat_vects):\n",
    "\n",
    "    K.set_epsilon(1e-2)\n",
    "    epsilon = K.epsilon()\n",
    "\n",
    "    x1, x2 = feat_vects\n",
    "\n",
    "    result = K.maximum(x=dot(inputs=[x1, x2], axes=1, normalize=True), y=epsilon)\n",
    "\n",
    "    return result\n",
    " \n",
    "# calculate l1_norm b/t feature vector outputs from base network\n",
    "def l1_distance(feat_vects):\n",
    "    \n",
    "    K.set_epsilon(1e-07)\n",
    "    epsilon = K.epsilon()\n",
    "\n",
    "    x1, x2 = feat_vects\n",
    "\n",
    "    result = K.maximum(x=K.sum(x=K.abs(x1-x2), axis=1, keepdims=True), y=epsilon)\n",
    "\n",
    "    return result\n",
    " \n",
    "\n",
    "# calculate l2_distance b/t feature vector outputs from base network\n",
    "def l2_distance(feat_vects):\n",
    "    \n",
    "    K.set_epsilon(1e-07)\n",
    "    epsilon = K.epsilon()\n",
    "\n",
    "    x1, x2 = feat_vects\n",
    "\n",
    "    result = K.sqrt(K.maximum(x=K.sum(x=K.square(x1 - x2), axis=1, keepdims=True), y=epsilon))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# create a siamese model that calculates similarity b/t two feature vectors\n",
    "def create_siamese_model(encoding_shape, similarity_metric):\n",
    "\n",
    "    encoding_a = Input(shape=encoding_shape, name='encoding_a')\n",
    "    encoding_b = Input(shape=encoding_shape, name='encoding_b')\n",
    "    \n",
    "    drop_a = Dropout(rate=0.6)(encoding_a)\n",
    "    drop_b = Dropout(rate=0.6)(encoding_b)\n",
    "\n",
    "    fc1_a = Dense(units=4096, activation='relu', kernel_regularizer=l2(l=0.0000), name='fc1_a')(drop_a)\n",
    "    fc1_b = Dense(units=4096, activation='relu', kernel_regularizer=l2(l=0.0000), name='fc1_b')(drop_b)\n",
    "\n",
    "    # fc1_a = Dropout(rate=0.3)(fc1_a)\n",
    "    # fc1_b = Dropout(rate=0.3)(fc1_b)\n",
    "\n",
    "    if similarity_metric == 'cosine':\n",
    "        distance = Lambda(function=cos_distance, name='cos_distance')([fc1_a, fc1_b])\n",
    "      \n",
    "    elif similarity_metric == 'l1':\n",
    "        distance = Lambda(function=l1_distance, name='l1_distance')([fc1_a, fc1_b])\n",
    "      \n",
    "    elif similarity_metric == 'l2':\n",
    "        distance = Lambda(function=l2_distance, name='l2_distance')([fc1_a, fc1_b])\n",
    "\n",
    "    prediction = Dense(units=1, activation='sigmoid', kernel_regularizer=l2(l=0.0000), name='sigmoid')(distance)\n",
    "\n",
    "    model = Model(inputs=[encoding_a, encoding_b], outputs=prediction, name='siamese_model')\n",
    "\n",
    "    return model\n",
    "\n",
    "# create siamese model\n",
    "encoding_shape = x_train_encoding.shape[2:]\n",
    "print(encoding_shape)\n",
    "\n",
    "cosine_model = create_siamese_model(encoding_shape, 'cosine')\n",
    "print(cosine_model.summary())\n",
    "\n",
    "l1_model = create_siamese_model(encoding_shape, 'l1')\n",
    "# print(l1_model.summary())\n",
    "\n",
    "l2_model = create_siamese_model(encoding_shape, 'l2')\n",
    "# print(l2_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4V_AfJEkMZp"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GKz11oNKkMZq"
   },
   "outputs": [],
   "source": [
    "def step_decay_schedule(lr_initial=0.001, decay=0.75, step_size=10):\n",
    "    def schedule(epoch):\n",
    "        return lr_initial * math.pow(decay, math.floor((1 + epoch) / step_size))\n",
    "\n",
    "    return LearningRateScheduler(schedule=schedule, verbose=1)\n",
    "\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "def create_callbacks(lr_type, wts_fn, enable_early_stopping=False, enable_save_wts=False):\n",
    "    cbks = []\n",
    "\n",
    "    # learning rate\n",
    "    if lr_type is 0:\n",
    "        lr_schedule = step_decay_schedule()\n",
    "        cbks.append(lr_schedule)\n",
    "\n",
    "    elif lr_type is 1:\n",
    "        reduce_lr_schedule = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                               factor=0.1,\n",
    "                                               patience=5,\n",
    "                                               min_lr=1e-6,\n",
    "                                               verbose=1)\n",
    "        cbks.append(reduce_lr_schedule)\n",
    "\n",
    "    # early stopping\n",
    "    if enable_early_stopping is True:\n",
    "        early_stopper = EarlyStopping(monitor='val_loss', patience=10)\n",
    "        cbks.append(early_stopper)\n",
    "\n",
    "    # model checkpoint\n",
    "    if enable_save_wts is True:\n",
    "        model_chpt = ModelCheckpoint(filepath=wts_fn,\n",
    "                                     monitor='val_loss',\n",
    "                                     verbose=1,\n",
    "                                     save_weights_only=True,\n",
    "                                     save_best_only=False,\n",
    "                                     period=10)\n",
    "\n",
    "        cbks.append(model_chpt)\n",
    "\n",
    "    return cbks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K02aPZ9sn99k"
   },
   "source": [
    "### Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wTMdjhaZoAd-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122400, 2048)\n",
      "Train on 122400 samples, validate on 13600 samples\n",
      "Epoch 1/5\n",
      "122400/122400 [==============================] - 73s 598us/step - loss: 0.2690 - acc: 0.8933 - lr: 0.0010 - val_loss: 0.3614 - val_acc: 0.8468 - val_lr: 0.0010\n",
      "Epoch 2/5\n",
      "122400/122400 [==============================] - 72s 591us/step - loss: 0.2666 - acc: 0.8941 - lr: 0.0010 - val_loss: 0.3629 - val_acc: 0.8495 - val_lr: 0.0010\n",
      "Epoch 3/5\n",
      "122400/122400 [==============================] - 72s 591us/step - loss: 0.2653 - acc: 0.8955 - lr: 0.0010 - val_loss: 0.3659 - val_acc: 0.8550 - val_lr: 0.0010\n",
      "Epoch 4/5\n",
      "122400/122400 [==============================] - 72s 592us/step - loss: 0.2623 - acc: 0.8978 - lr: 0.0010 - val_loss: 0.3749 - val_acc: 0.8507 - val_lr: 0.0010\n",
      "Epoch 5/5\n",
      "122400/122400 [==============================] - 72s 591us/step - loss: 0.2640 - acc: 0.8963 - lr: 0.0010 - val_loss: 0.3651 - val_acc: 0.8498 - val_lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# create callbacks\n",
    "lr_type = 3  # 0=step decay, 1=val_loss decay\n",
    "cosine_cbks = create_callbacks(lr_type, 'traingen_wts13.h5', True, False)\n",
    "\n",
    "# training setup\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "optim = RMSprop(lr=1e-3)\n",
    "lr_metric = get_lr_metric(optim)\n",
    "\n",
    "cosine_model.compile(loss=\"binary_crossentropy\", optimizer=optim, metrics=['accuracy',lr_metric])\n",
    "# K.clear_session()\n",
    "\n",
    "print(x_train_encoding[:, 1].shape)\n",
    "hist_cosine = cosine_model.fit(x=[x_train_encoding[:, 0], x_train_encoding[:, 1]],\n",
    "                               y=y_train,\n",
    "                               batch_size=batch_size,\n",
    "                               epochs=n_epochs,\n",
    "                               validation_data=([x_val_encoding[:, 0], x_val_encoding[:, 1]], y_val),\n",
    "                               shuffle=True,\n",
    "                               verbose=1,\n",
    "                               callbacks=cosine_cbks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_model.save_weights('traingen_wts13.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6aUObcuoiip"
   },
   "source": [
    "### L1 Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hocXoIfmok4R"
   },
   "outputs": [],
   "source": [
    "# create callbacks\n",
    "lr_type = 3  # 0=step decay, 1=val_loss decay\n",
    "l1_cbks = create_callbacks(lr_type, 'traingen_wts8.h5', False, True)\n",
    "\n",
    "# training setup\n",
    "batch_size = 32\n",
    "n_epochs = 20\n",
    "optim = RMSprop(lr=1e-2)\n",
    "lr_metric = get_lr_metric(optim)\n",
    "\n",
    "l1_model.compile(loss=\"binary_crossentropy\", optimizer=optim, metrics=['accuracy',lr_metric])\n",
    "# K.clear_session()\n",
    "\n",
    "print(x_train_encoding[:, 1].shape)\n",
    "hist_l1 = l1_model.fit(x=[x_train_encoding[:, 0], x_train_encoding[:, 1]],\n",
    "                          y=y_train,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=n_epochs,\n",
    "                          validation_data=([x_val_encoding[:, 0], x_val_encoding[:, 1]], y_val),\n",
    "                          shuffle=True,\n",
    "                          verbose=1,\n",
    "                          callbacks=l1_cbks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jxp1m2Mwo6Jm"
   },
   "source": [
    "### L2 Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xEjMbWYlo6p2"
   },
   "outputs": [],
   "source": [
    "# create callbacks\n",
    "lr_type = 3  # 0=step decay, 1=val_loss decay\n",
    "l2_cbks = create_callbacks(lr_type, 'traingen_wts9.h5', False, True)\n",
    "\n",
    "# training setup\n",
    "batch_size = 32\n",
    "n_epochs = 20\n",
    "optim = RMSprop(lr=1e-2)\n",
    "lr_metric = get_lr_metric(optim)\n",
    "\n",
    "l2_model.compile(loss=\"binary_crossentropy\", optimizer=optim, metrics=['accuracy',lr_metric])\n",
    "# K.clear_session()\n",
    "\n",
    "print(x_train_encoding[:, 1].shape)\n",
    "hist_l2 = l2_model.fit(x=[x_train_encoding[:, 0], x_train_encoding[:, 1]],\n",
    "                          y=y_train,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=n_epochs,\n",
    "                          validation_data=([x_val_encoding[:, 0], x_val_encoding[:, 1]], y_val),\n",
    "                          shuffle=True,\n",
    "                          verbose=1,\n",
    "                          callbacks=l2_cbks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LAvxWB4OkMZ9"
   },
   "source": [
    "## Plot Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JHnxcptekMaB"
   },
   "outputs": [],
   "source": [
    "def plot_training_metrics(title, x_axis_label, y_axis_label, y, x=None):\n",
    "    \n",
    "    if x is None:\n",
    "        plt.plot(y[0])\n",
    "        plt.plot(y[1])\n",
    "    else:\n",
    "        plt.plot(x,y[0])\n",
    "        plt.plot(x,y[1])\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.ylabel(y_axis_label)\n",
    "    plt.xlabel(x_axis_label)\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xu981wX4kMaI"
   },
   "outputs": [],
   "source": [
    "# Loss vs. epochs - Cosine\n",
    "plot_training_metrics(title='Model Loss',x_axis_label='Epoch',y_axis_label='Loss',\n",
    "                     y=[hist_cosine.history['loss'], hist_cosine.history['val_loss']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XAJ2-QMXkMaX"
   },
   "outputs": [],
   "source": [
    "# Loss vs. epochs - L1 Norm\n",
    "plot_training_metrics(title='Model Loss',x_axis_label='Epoch',y_axis_label='Loss',\n",
    "                     y=[hist_l1.history['loss'], hist_l1.history['val_loss']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Uvl-xo9Arbh7"
   },
   "outputs": [],
   "source": [
    "# Loss vs. epochs - L2 Norm\n",
    "plot_training_metrics(title='Model Loss',x_axis_label='Epoch',y_axis_label='Loss',\n",
    "                     y=[hist_l2.history['loss'], hist_l2.history['val_loss']])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "siamese_train.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
