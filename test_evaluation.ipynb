{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_evaluation.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "RjIoeIAlheCz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a2f6db4-304f-43a6-8988-bbc2f1df1d86",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529973466850,
          "user_tz": 240,
          "elapsed": 2329,
          "user": {
            "displayName": "Haris Khan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "114865499777940001208"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import cv2 as cv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.applications import inception_v3, inception_resnet_v2, mobilenet, resnet50, vgg16\n",
        "from keras.layers import dot, Dense, GlobalAveragePooling2D, Lambda, Input\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import RMSprop, SGD"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "dQrilx_Uhpyi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import test image csv information"
      ]
    },
    {
      "metadata": {
        "id": "H1Unwx9Whki3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "data_dir = 'data'\n",
        "test_dir = 'test'\n",
        "\n",
        "test_dict = dict()\n",
        "\n",
        "print(\"Test folder: %s\" %(os.path.join(data_dir,test_dir)))\n",
        "\n",
        "for i, fn in enumerate(glob(os.path.join(data_dir,test_dir,'*.jpg'))):\n",
        "    # print(fn)\n",
        "    test_dict[i] = fn\n",
        "\n",
        "print(\"# of test images: %s\" %str(len(test_dict)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pbEgqcsEh1Mo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import whale database information from training set"
      ]
    },
    {
      "metadata": {
        "id": "DptosckhhvFG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train_csv = 'train.csv'\n",
        "train_dir = 'train'\n",
        "df = pd.read_csv(filepath_or_buffer=os.path.join(data_dir,train_csv))\n",
        "\n",
        "train_classes = df.groupby(\"Id\").size()\n",
        "print(\"\\n# of unique database classes: %d\" %train_classes.shape[0])\n",
        "\n",
        "# create dictionary mapping labels to image filenames\n",
        "db_img_filenames = df[\"Image\"].tolist()\n",
        "db_labels = df[\"Id\"].tolist()\n",
        "db = dict(zip(db_labels,db_img_filenames))\n",
        "\n",
        "# remove new_whale key from dictionary\n",
        "'''for i,key in enumerate(db.keys()):\n",
        "    if key == \"new_whale\":\n",
        "        print(\"New whale_idx: %d\" %i)'''\n",
        "\n",
        "del db[\"new_whale\"]\n",
        "\n",
        "# print(len(db))\n",
        "\n",
        "# create dictionary mapping label index to label\n",
        "labels_dict = {}\n",
        "for i, label in enumerate(db.keys()):\n",
        "    labels_dict[i]=label\n",
        "\n",
        "# print(str(len(labels_dict)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rjVEel1EiE0r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Siamese Network"
      ]
    },
    {
      "metadata": {
        "id": "HA60_gbPh9pc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "img_shape = (224,224,3)\n",
        "weights_dir = 'weights'\n",
        "weights_filename = ''\n",
        "base_id = 4  # 0 = Inception-v3, 1 = MobileNet, 2 = Inception-ResNet-v2, 3= ResNet50, 4=VGG16\n",
        "enable_saved_wts = False\n",
        "\n",
        "# choose pre-trained base network that generates a (n,1) feature vector for an input image\n",
        "def create_pre_trained_base(input_shape, base_id, num_train_layers):\n",
        "    # Inception-v3\n",
        "    if base_id is 0:\n",
        "        base = inception_v3.InceptionV3(input_shape=input_shape, weights='imagenet', include_top=False)\n",
        "        base_name = 'Inception-V3'\n",
        "\n",
        "    # MobileNet\n",
        "    elif base_id is 1:\n",
        "        base = mobilenet.MobileNet(input_shape=input_shape, weights='imagenet', include_top=False)\n",
        "        base_name = 'MobileNet'\n",
        "\n",
        "    # Inception-ResNet-v2\n",
        "    elif base_id is 2:\n",
        "        base = inception_resnet_v2.InceptionResNetV2(input_shape=input_shape, weights='imagenet', include_top=False)\n",
        "        base_name = 'InceptionResNet-v2'\n",
        "\n",
        "    # ResNet50\n",
        "    elif base_id is 3:\n",
        "        base = resnet50.ResNet50(input_shape=input_shape, weights='imagenet', include_top=False)\n",
        "        base_name = 'ResNet50'\n",
        "\n",
        "    # VGG16\n",
        "    elif base_id is 4:\n",
        "        base = vgg16.VGG16(input_shape=input_shape, weights='imagenet', include_top=False)\n",
        "        base_name = 'VGG16'\n",
        "\n",
        "    print(\"\\nBase Network: %s\" % base_name)\n",
        "\n",
        "    top = GlobalAveragePooling2D()(base.output)\n",
        "\n",
        "    # freeze all layers in the base network\n",
        "    for layer in base.layers[0:len(base.layers) - num_train_layers]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model = Model(inputs=base.input, outputs=top, name='base_model')\n",
        "\n",
        "    return model\n",
        "\n",
        "# calculate cosine distance b/t feature vector outputs from base network\n",
        "def cos_distance(feat_vects):\n",
        "\n",
        "    K.set_epsilon(1e-07)\n",
        "    epsilon = K.epsilon()\n",
        "\n",
        "    x1, x2 = feat_vects\n",
        "\n",
        "    result = K.maximum(x=dot(inputs=[x1, x2], axes=1, normalize=True), y=epsilon)\n",
        "\n",
        "    return result\n",
        "\n",
        "# create a siamese model that calculates similarity b/t two feature vectors\n",
        "def create_siamese_model(base, input_shape):\n",
        "    input_a = Input(shape=input_shape, name='input_a')\n",
        "    input_b = Input(shape=input_shape, name='input_b')\n",
        "\n",
        "    encoding_a = base(input_a)\n",
        "    encoding_b = base(input_b)\n",
        "\n",
        "    fc1_a = Dense(units=2048, activation='relu', kernel_regularizer=None, name='fc1_a')(encoding_a)\n",
        "    fc1_b = Dense(units=2048, activation='relu', kernel_regularizer=None, name='fc1_b')(encoding_b)\n",
        "\n",
        "    distance = Lambda(function=cos_distance, name='cos_distance', )([fc1_a, fc1_b])\n",
        "\n",
        "    prediction = Dense(units=1, activation='sigmoid', kernel_regularizer=l2(l=0.0000), name='sigmoid')(distance)\n",
        "\n",
        "    model = Model(inputs=[input_a, input_b], outputs=prediction, name='siamese_model')\n",
        "\n",
        "    if enable_saved_wts is True:\n",
        "      model.load_weights(filepath=os.path.join(weights_dir,weights_filename),by_name=True)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# create siamese model\n",
        "base_model = create_pre_trained_base(input_shape=img_shape, base_id=base_id, num_train_layers=3)\n",
        "# print(base_model.summary())\n",
        "siamese_model = create_siamese_model(base_model, img_shape)\n",
        "print(siamese_model.summary())\n",
        "\n",
        "# compile model\n",
        "siamese_model.compile(optimizer=RMSprop(),loss=\"binary_crossentropy\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o69F6kcUigxx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "pJP5zfifijTr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# load image from disk\n",
        "def load_img(fn):\n",
        "\n",
        "    # print(os.path.join(data_dir,fn))\n",
        "\n",
        "    img = cv.imread(filename=os.path.join(fn),flags=0)\n",
        "    img = cv.resize(src=img, dsize=img_shape, dst=img)\n",
        "    img = np.array(img, dtype=np.float32)\n",
        "    img /=  255\n",
        "    img = np.stack((img,img,img),axis=-1)\n",
        "\n",
        "    return img\n",
        "\n",
        "# return top-N database predictions\n",
        "def find_topN(predictions, N):\n",
        "\n",
        "    # find N indices with the highest similarity predictions\n",
        "    idxs_topN = np.argsort(predictions)[-N:]\n",
        "\n",
        "    labels = []\n",
        "\n",
        "    # check if maximum prediction is <= 0.5. If so, assign \"new_whale\" class to top-1 prediction\n",
        "    idx_max = idxs_topN[N-1]\n",
        "    if predictions[idx_max] <= 0.5:\n",
        "        labels.append(\"new_whale\")\n",
        "        stop_idx = 0\n",
        "    else:\n",
        "        # labels.append(labels_dict[idx_max])\n",
        "        stop_idx = -1\n",
        "\n",
        "    for i in range(N-1,stop_idx,-1):\n",
        "        idx = int(idxs_topN[i])\n",
        "        labels.append(labels_dict[idx])\n",
        "\n",
        "    return labels\n",
        "\n",
        "n_test = len(test_dict)\n",
        "n_db = len(db)\n",
        "results = np.zeros(shape=(n_test,n_db),dtype=np.float32)\n",
        "final_predictions = []\n",
        "\n",
        "# iterate through all test image and db combinations\n",
        "for i, test_img_fn in enumerate(test_dict.values()[0:2]):\n",
        "\n",
        "    # import a single test image\n",
        "    test_img = load_img(test_img_fn)\n",
        "    offset = len(test_dir)+len(data_dir)+2\n",
        "\n",
        "    # calculate similarity b/t test image and all db images\n",
        "    for j, query_fn in enumerate(db.values()[0:5]):\n",
        "\n",
        "        db_img = load_img(os.path.join(data_dir,train_dir,query_fn))\n",
        "        eval_pair = [[test_img,db_img]]\n",
        "        eval_pair = np.array(eval_pair)\n",
        "        # print(eval_pair.shape)\n",
        "        prediction = siamese_model.predict(x=[eval_pair[:,0],eval_pair[:,1]], batch_size=1, verbose=0)\n",
        "\n",
        "        results[i][j] = prediction[0]\n",
        "\n",
        "    topN = find_topN(results[i],3)\n",
        "    final_predictions.append([test_img_fn[offset:],topN])\n",
        "\n",
        "    print(\"\\nImage %d: %s\" %(i,test_img_fn[offset:]))\n",
        "    print(\"Top N Predictions: %s\" %str(topN))\n",
        "\n",
        "print(str(results))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gT1hq-0XjNBm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save top-5 predictions to csv"
      ]
    },
    {
      "metadata": {
        "id": "NiQx2Z4zjRnD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "save_dir = 'results'\n",
        "\n",
        "csv_fn = 'results.csv'\n",
        "\n",
        "with open(os.path.join(save_dir,csv_fn), 'w') as f:\n",
        "    writer = csv.writer(f, delimiter=',')\n",
        "\n",
        "    for name, labels in final_predictions:\n",
        "        l_concat = \" \".join(labels)\n",
        "        print(l_concat)\n",
        "        writer.writerow([name,l_concat])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}